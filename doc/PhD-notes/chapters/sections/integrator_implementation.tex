\section{Integrator implementation}\label{sec:integrator-implementation}

In this section a brief outline of the integrator designed and implemented for 
the current Thesis is given. The method used, is a \emph{variable step}, 
\emph{variable order} Adams-Bashforth-Moulton \gls{pece} algorithm. The 
implementation is based on the developments of \cite{Shampine1975}, which up to 
date constitute one of the most robust \gls{ode} solvers solvers, given the 
problem constraints (non stiff \gls{ode} systems of 1\textsuperscript{st} order).

According to \cite{Shampine1975}, use of Adams methods are most advantageous for 
problems in which:
\begin{itemize}
    \item function evaluations are expensive,
    \item moderate to high accuracy is requested,
    \item many output points are required
\end{itemize}

In \gls{pod}, the cost of calling the derivative function $\bm{f}(t, \bm{y}(t))$ 
is quite heavy and repeated calls can make the integrator inefficient. Hence, care 
must be taken to limit such computations. The Adams methods, when carefully used, 
are more efficient in this respect than any other method being used today to 
solve general \gls{ode} (\cite{Shampine1975}). To achieve this efficiency though, 
variable step size and order should be used. This however incurs an extra overhead, 
since to make such decisions, it is necessary to estimate the errors that are, 
or would be, incurred for various step sizes and orders. To tackle this problem, 
most steps are taken in groups with a constant step size and constant order.

\begin{figure}
  \centering
  \input{tikz/sgode}
  \caption{Flow chart of the implemented \gls{pece} integrator, based 
    on \cite{Shampine1975}.}
  \label{fig:sgode}
\end{figure}

For a varying step size, a wise choice of stored variables and coefficients 
should be made to accomodate efficiency. Since, the algorithm will be using divided 
and backward differences, this choice includes the variables

\begin{gather}
  h_i  = x_i - x_{i-1}, \label{eq:sg4a}\\
  s    = \frac{x - x_n}{h_{n+1}}, \label{eq:sg4b}\\
  \psi _i (n+1)   = h_{n+1} + h_{n} + \dots + h_{n+2-i} \text{ } i \ge 1, \label{eq:sg4c}\\
  \alpha _i (n+1) = \frac{h_{n+1}}{\psi _i (n+1)} \text{ } i \ge 1, \label{eq:sg4d}\\
  \beta _1 (n+1)  = 1, \label{eq:sg4e}\\
  \beta _i (n+1)  = \frac{\psi _1 (n+1) \psi _2 (n+1) \dots \psi _{i-1}(n+1)}{\psi _1 (n) \psi _2 (n) \dots \psi _{i-1}(n)} \text{ } i > 1, \label{eq:sg4f}\\
  \phi _1(n) = f[x_n] = f_n, \label{eq:sg4g}\\
  \phi _i(n) = \psi _1 (n) \psi _2 (n) \dots \psi _{i-1}(n) f[x_n,x_{n-1},\dots ,x_{n-i+1}], \text{ } i > 1 \label{eq:sg4h}
\end{gather}
where $h_i$ are the step size, 
$\psi _i(n+1) = h_{n+1} + h_{n} + \dots + h_{n+2-i} = x_{n+1}-x{n+1-i}$ are the 
sums of the step sizes, $\phi _{i}(n)$ are the modified divided differences (which 
reduce to backward differences for constant step sizes), $s$ is a normalized variable 
taking value in the range $[0,1]$ as x runs from $x_n$ to $x_{n+1}$ and $\alpha _i$ 
and $\beta _i$ are intermediate coefficients used for the computations.

Through \autoref{eq:sg4a} to \autoref{eq:sg4h}, a change from value $n$ to $n+1$ 
can be performed via the recusrion for $i=2,3,\dots$
\begin{equation}
  \begin{aligned}
    \psi _i(n+1)   &= \psi _{i-1}(n) + h_{n+1} \\
    \beta _i(n+1)  &= \beta _{i-1}(n+1) \frac{\psi _{i-1}(n+1)}{\psi _{i-1}(n)}
  \end{aligned}
\end{equation}
with initial conditions for $i=1$
\begin{equation}
  \begin{aligned}
    \psi _1(n+1)   &= h_{n+1} \\
    \beta _1(n+1)  &= 1
  \end{aligned}
\end{equation}

It is worth noting that if the step size is constant, \autoref{eq:sg4a} through 
\autoref{eq:sg4h} are simplified to
\begin{gather}
  s = \frac{x-x_n}{h_{n+1}} , \label{eq:sg4Ab}\\
  \psi _i (n+1)   = ih \label{eq:sg4Ac}\\
  \alpha _i (n+1) = \frac{1}{i} \label{eq:sg4Ad}\\
  \beta_i (n+1)   = 1 \label{eq:sg4Ae}\\
  \phi _i(n)      = \nabla ^{i-1} f_n, \label{eq:sg4g}
\end{gather}

The expression of the polynomial $P_{k,n}(x)$ in \autoref{eq:sg52a} and \autoref{eq:sg52b} 
can be written as
\begin{equation}\label{eq:sq57}
  P_{k,n}(t) = \sum_{i=1}^{k} c_{i,n} (s) \phi ^{*}_{i}(n)
\end{equation}
where
\begin{align}
  c_{i,n}(s) &= \begin{cases}
    1                                    & i=1 \\
    \frac{s h_{n+1}}{\psi _1 (n+1)} = s  & i=2 \\
    \left(\frac{sh_{n+1}}{\psi _1(n+1)}\right)
    \left(\frac{sh_{n+1} + \psi _1(n)}{\psi _2(n+1)}\right)
    \dots
    \left(\frac{sh_{n+1} + \psi _{i-2}(n)}{\psi _{i-1}(n+1)}\right) & i \ge 3
  \end{cases}
  \label{eq:sg56} \\
  \phi ^{*}_{i}(n) &= \beta _i (n+1) \phi _i (n) \label{eq:sg56b}
\end{align}
The derivative in \autoref{eq:sg52b}, is then
\begin{equation}\label{eq:sg57b}
   P_{k,n}(t_{n+1}) = {p'}_{n+1} = \sum_{i=1}^{k} \phi ^{*}_{i}(n)
\end{equation}
and the (approximate) solution at $t_{n+1}$, \autoref{eq:sg52a} is
\begin{equation}\label{eq:sg58}
  p_{n+1} = y_n + h_{n+1} \sum_{i=1}^{k} \phi ^{*}_{i}(n) \int_{0}^{1} c_{i,n}(s) \,ds
\end{equation}
As we saw, for constant step size $h$, \autoref{eq:sg58} reduces to
\begin{equation}\label{eq:sg58b}
  p_{n+1} = y_n + h \sum_{i=1}^{k} \gamma _{i-1} \nabla ^{i-1} f_n 
\end{equation}
To compute the integral in \autoref{eq:sg58}, the quantity $g_{i,q}$ is 
introduced, defined as $g_{i,q} = (q-1)! c_{i,n}^{(-q)}(1)$. According to 
\cite{Shampine1975}, the integral can now be evaluated via the formula
\begin{equation}
    \int_{0}^{1} c_{i,n}(s) \,ds = \sum_{i=1}^{k} g_{i,1} \phi ^{*}_{i}(n)
\end{equation}
so that \autoref{eq:sg58} can be numerically computed from
\begin{equation}\label{eq:sg511}
  p_{n+1} = y_n + h_{n+1}  \sum_{i=1}^{k} g_{i,1} \phi ^{*}_{i}(n)
\end{equation}
The coefficients $g_{i,q}$ follow the recursion formulas
\begin{equation}\label{eq:sg510}
  g_{i,q} = \begin{cases}
    \frac{1}{q} & i=1 \\
    \frac{1}{q(q+1)} & i=2 \\
    g_{i-1,q} - \alpha _{i-1}(n+1) g_{i-1,q+1} & i \ge 3
  \end{cases}
\end{equation}

In the formulas presented here, a series of simplifications can be made if we 
consider a constant step size $h$. These are implemented in the source code for 
the integrator. The interested readed can find details in \cite{Shampine1975}.

\iffalse
\cite{Shampine1975}:
The overhead in these codes is rather high, partly because of the methods
and partly because of the features they provide. For example, DE detects
and deals with discontinuities; it detects and deals with moderate stiffness;
it detects and tells the user of severe stiffness; it detects requests for very
high accuracy and switches on propagated roundoff controls; it detects
requests for more accuracy than is possible on the machine being used and
tells the user what is possible; it is, for practical purposes, independent
of the number and location of output points; it monitors the work being
expended; it allows the user to change direction without restarting; and
it is extremely efficient in terms of function evaluations. 
\fi

\subsection{The Predictor}\label{ssec:integrator-predictor}
The predictor part of the \gls{pece} integrator follows the Adams-Bashforth 
method (see \autoref{ssec:adams-method}). In general, a $k$th order Adams-Bashforth 
predictor at $t_n$ is defined by the expression 
\begin{equation}\label{eq:sg51}
  y_n + \int_{t_n}^{t} P_{k,n}(t) \,dt
\end{equation}
where $P_{k,n}(t)$ satisfies the interpolation conditions
\begin{equation}\label{eq:sg51b}
  P_{k,n}(t_{n+1-j}) = \bm{f}_{n+1-j} \text{ for } j=1,\dots ,k
\end{equation}
\autoref{eq:sg51} is used to predict both the solution and its derivative at 
$t_{n+1}$
\begin{align}
  p_{n+1}    &= y_n + \int_{t_n}^{t_{n+1}}  P_{k,n} (t) \,dt \label{eq:sg52a}\\
  {p'}_{n+1} &= P_{n,k}(t_{n+1}) \label{eq:sg52b}
\end{align}
The implementation computes the above values using \autoref{eq:sg511} and 
\autoref{eq:sg57b} using a series of intermediate variables and coefficients (
as described in \autoref{sec:integrator-implementation}). To promote efficiency, 
these coefficients are stacked in contiguous memory, devided in blocks based on 
their usage in the algorithm, to minimize as much as possible cache misses.

\subsection{The Corrector}\label{ssec:integrator-corrector}
The corrector part of the \gls{pece} integrator is based on the Adams-Moulton 
algorithm. In this step, $p_{n+1}$ is ``corrected'' and all relevant values 
needed for the next step are computed and stored. The corrector implemented here, 
is one order higher than the predictor. In general, the solution and its derivative, 
using an Adams-Moulton method, is given by
\begin{equation}\label{eq:sg5cor1}
  y_{n+1} = y_n + \int_{t_n}^{t_{n+1}} P^{*}_{k+1,n}(t) \,dt
\end{equation}
and
\begin{equation}\label{eq:sg51b}
  f_{n+1} = f(t_{n+1}, y_{n+1})
\end{equation}
where 
\begin{align}
  P^{*}_{k+1,n}(t_{n+1-j}) &= f_{n+1-j} \mbox{ for } j=1,2,\dots ,k \\
  P^{*}_{k+1,n}(t_{n+1})   &= f^{p}_{n+1} = f(t_{n+1}, p_{n+1})
\end{align}
The corrector polynomial interpolates the same set of values as the predictor, plus 
the additional value $f^{p}_{n+1}$. Using the divided differences formulation, 
$P^{*}_{k+1,n}$ can be computed from $P_{k,n}$ using one extra term. Using the 
notation introduced in \autoref{sec:integrator-implementation}, the polynomial 
can be given by
\begin{equation}
  P^{*}_{k+1,n}(t) = P_{n,k}(t) + c_{k+1,n}(s) \phi ^{p}_{k+1}(n+1)
\end{equation}
and after integration, the solution and derivative function can be shown to be 
(\cite{Shampine1975})
\begin{align}
  y_{n+1} &= p_{n+1} + h_{n+1} g_{k+1,1} \phi ^{p}_{k+1} (n+1) \label{eq:sg513a} \\
  f_{n+1} &= f(t_{n+1}, y_{n+1}) \label{eq:sg513b}
\end{align}
\autoref{eq:sg513a} allows the computation of the ``corrected'' solution using the 
work done at the corrector phase, plus the term $g_{k+1,1}$; the latter is computed 
along with the $g_{i,1}$, needed in the prediction (see \autoref{eq:sg511}). Note 
also that changing the term $g_{k+1,1}$ to $g_{k,1}$, yields the solution of 
order $k$ (\cite{Shampine1975}), a fact used to assist the estimation of the error 
(here the notation $y_{n+1}(k)$ is used to distinguish between the order $k$ and 
solution the order $k+1$ as given in \autoref{eq:sg513a})
\begin{equation}\label{eq:sg513ak}
  y_{n+1}(k) = p_{n+1} + h_{n+1} g_{k,1} \phi ^{p}_{k+1} (n+1)
\end{equation}
In the case of constant step size $h$, \autoref{eq:sg513a} and \autoref{eq:sg513ak} 
reduce to
\begin{align}
  y_{n+1}    &= p_{n+1} + h \gamma _k \nabla ^{k} f^{p}_{n+1} \\
  y_{n+1}(k) &= p_{n+1} + h \gamma _{k-1} \nabla ^{k} f^{p}_{n+1}
\end{align}
The terms $\phi ^{p}_{i} (n+1)$ and $\phi _{i} (n+1)$ can be computed via recursion, 
given that the divided difference approach is followed, from (\cite{Shampine1975})
\begin{align}
  \phi ^{p}_{i+1} (n+1) &= \phi ^{p}_{i} (n+1) - \phi ^{*}_{i} (n) \mbox{  for } i=1,2,\dots ,k \label{eq:sg514}\\
  \phi _{i+1} (n+1)     &= \phi _{i} (n+1) - \phi ^{*}_{i} (n)     \mbox{  for } i=1,2,\dots ,k \label{eq:sg515}
\end{align}
with initial conditions
\begin{align}
  \phi ^{p}_{1} (n+1) = f_{n+1}^{p} \\
  \phi _{1} (n+1) = f_{n+1}
\end{align}
According to \cite{Shampine1975}, despite the straightforwardness of these recursive 
relations, they lack storage efficiency. This can be corrected using an alternate way 
to compute these terms. Introducing the modified divided differences $\phi ^{e}_{i}$, 
that make us of the value ${p'}_{n+1}$ at $t_{n+1}$ (that is extend the differences 
$\phi _i(n)$ based on the sequence $x_n, x_{n-1}, \dots$ to the values at 
$x_{n+1}, x_{n}, x_{n-1} \dots$)
\begin{equation}\label{eq:sg518}
  \phi ^{e}_{i}(n+1) = \phi ^{e}_{i+1} (n+1) + \phi^{*}_{i} (n)
\end{equation}
which can be used to generate the $\phi ^{e}_{i}(n+1)$ terms in the sequence 
$i=k, k-1, \dots ,1$. Now, \autoref{eq:sg514} and \autoref{eq:sg515} can be written as
\begin{align}
  \phi ^{p}_{i} (n+1) &= \phi ^{e}_{i} (n+1) - \left( f^{p}_{n+1} - \phi ^{e}_{1} (n+1) \right) \label{eq:sg519}\\
  \phi _{i} (n+1)     &= \phi ^{e}_{i} (n+1) - \left( f^{p}_{n+1} - \phi ^{e}_{1} (n+1) \right) \label{eq:sg520}
\end{align}

\begin{figure}
  \centering
  \input{tikz/adams-pece}
  \caption{Schematic representation of the Adams \gls{pece} integrator, based 
    on \cite{Shampine1975}.}
  \label{fig:adams-pece}
\end{figure}

\subsection{Error Estimation, Order and Step Size}\label{ssec:integrator-error-estimation}
Error estimation in the integration process plays a crucial role, since it allows 
the algorithm to adjust the step size and order accordingly, so as to preserve 
efficiency and accuracy. Local error can be computed before the final step of the 
\gls{pece} algorithm, aka after the corrector, for orders $k-2$, $k-1$, $k$ and 
$k+1$. Hence, if the step is unsuccessful, the final evaluation is not performed, 
and a change of the order is examined.

In the source code the quantity $ERK$ is used to make decisions on the step size. 
This quantity was introduced by \cite{Shampine1975} and is given by
\begin{equation}
  ERK = \norm{h \left( g_{k+1,1} - g_{k,1} \right) \sigma _{k+1}(n+1) \phi ^{p}_{k+1}(n+1)}
\end{equation}
In reality, $ERK$ estimates what the local error at $x_{n+1}$ would have been, 
had the preceding steps been of constant size $h$. Given a successeful step, the 
next step (to $x_{n+2}$) of size $rh$, will have a local error of $r^{k+1} ERK$. 
If on contrary the step was to be repeated with a step size of $rh$, then again 
its error estimate would be $r^{k+1} ERK$.

For order selection, three further quantities are introduced, based on \cite{Shampine1975}
\begin{align}
  ERKM1 &= \norm{h \left( g_{k+1,1} - g_{k,1} \right) \sigma _{k}(n+1) \phi ^{p}_{k}(n+1)} \\
  ERKM2 &= \norm{h \left( g_{k+1,1} - g_{k,1} \right) \sigma _{k-1}(n+1) \phi ^{p}_{k-1}(n+1)}\\
  ERKP1 &= \norm{h \left( g_{k+1,1} - g_{k,1} \right) \phi _{k+2}(n+1)}
\end{align}
The first two, estimate what the local error would have been at $x_{n+1}$ had these 
steps be taken with an order of $k-1$ and $k-2$ respectively. $ERKP1$ is an estimate 
of the local error at $x_{n+2}$. All of the estimates assume a step size $h$. 
The philosophy of the order selection is to change the order only if the predicted 
error is reduced and if there is a trend in the terms. In general, lower order 
formulas have better stability properties (\cite{Shampine1975}), and the algorithm 
is so designed as to prefer lower orders. The maximum order limit within the 
algorithm is 12.

After the order selection, the algorithm tries to select the optimal step size $rh$. 
The value of $r$ is initialy estimated by
\begin{equation}\label{eq:sg71}
  r = \left( \frac{0.5 \epsilon}{ERK} \right) ^{\frac{1}{k+1}}
\end{equation}
and restrict increase of $r$ to factors of two. Hence, if the inequality 
$0.5 \epsilon \ge 2^{k+1}ERK$, holds, the step size is doubled. If this is not 
possible, then the inequality $0.5 \epsilon \ge ERK$ is checked, to see if a constant 
step size can be retained. Keeping a constant step size (when a factor of two 
increase is not feasible) offers efficiency, simplicity, stability and a more 
realistic error estimation. If however the latter inequality demands so, a decrease 
of step size is performed by a factor inbetween 0.5 to 0.9, computed via \autoref{eq:sg71}.
The value of $\epsilon$ is user defined.



%An estimate of local error used in the algorithm and introduced by \cite{Shampine1975}, 
%is given by
%\begin{equation}
%  ERR = \norm{h_{n+1} \left( g_{k+1,1} - g_{k,1} \right) \phi ^{p}_{k+1}(n+1)} 
%    \equiv \norm{le_{n+1}(k)}
%\end{equation}
%When trying to make step, e.g. from $x_n$ to $x_{n+1}$, the error estimate is used 
%to either predict the next step's error (via 
%$\norm{le_{n+2}(k)} \equiv \norm{h_{n+2} \left( g_{k+1,1} - g_{k,1} \right) \phi ^{p}_{k+1}(n+2)}$), or repeat the step with a smaller step size and possibly a different order. 
%The algorith tries to find the largest step size for which the predicted local 
%error is acceptable.

\iffalse
Obviously we must determine an easy way to integrate the interpolating
polynomial. We start with the polynomial in divided difference form
\begin{equation}\label{eq:sg53}
  P_{k,n}(x) = f[x_n] 
    + (x-x_n)f[x_n,x_{n-1}
    + (x-x_n)(x-x_{n-1})f[x_n,x_{n-1},x_{n-2}]
    + \dots 
    + (x-x_n)(x-x_{n-1})\dots (x-x_{n-k+2})f[x_n,x_{n-1},x_{n-2},\dots ,x_{n-k-1}]
\end{equation}
where
\begin{equation}
  \begin{aligned}
    f[x_0] & & \equiv f(x_0) \text{ and}\\
    f[x_0,x_1,\dots ,x_n] &= \frac{f[x0,\dots ,x_{n-1}]-f[x1,\dots ,x_n]}{x_0 - x_n}
  \end{aligned}
\end{equation}
and we assume that $n \ge 1$.
Because we intend to vary the step size, we will take a code variables the take the 
step sizes $h_i = x_i - x_{i-1}$ as fundamental variables along with their sums 
$\psi (n+1) = h_{n+1} + h_{n} + \dots + h_{n+2-i} = x_{n+1} - x_{n+1-i}$. The 
divided differences will be modified so that they reduce to backward differences 
when the step sizes are constant. Using these modified divided differences $\phi _i (n)$ 
leads to economical, simple formulas. For integrations like the one in \autoref{eq:sg52a} 
we again find it convenient to introduce a normalized variable $s$ by $x=x_n +sh_{n+1}$ 
so that $s$ runs from 0 to 1 as $x$ runs from $x_n$ to $x_{n+1}$. We will also need 
two more fundamental quantities $\alpha _i (n+1)$ and $\beta _i (n+1)$

It is worth noting that if the step size is a constant $h$, then 
\begin{equation}
  \begin{aligned}
    \psi _i(n+1)   &= ih \\
    \alpha _i(n+1) &= \frac{1}{i} \\
    \beta _i(n+1)  &= 1 \\
    \phi _i(n)     &= \nabla ^{i-1} f_{n}
  \end{aligned}
\end{equation}
$\phi _i (n)$ values can be computed efficiently using divided differences. For 
the $\psi _i$ and $\beta _i$ values, the following formulas are used for recursion 
\begin{equation}
  \begin{aligned}
    \psi _i (n+1)  &= \psi _{i-1} (n) + h_{n+1} \\
    \beta _i (n+1) &= \beta _{i-1}(n+1) \frac{\psi _{i-1} (n+1)}{\psi _{i-1}(n)} \\
  \end{aligned}
\end{equation}
starting with the initial values
\begin{equation}
  \begin{aligned}
    \psi _1 (n+1) &= h_{n+1} \\
    \beta _1(n+1) &= 1
  \end{aligned}
\end{equation}

The interpolating polynomial, analytical work can be done (see \cite{Shampine1975}) 
\begin{equation}\label{eq:sg57}
  P_{k,n} (x) = \sum_{i=1}^{k} c_{i,n} (s) \phi ^{*}_{i} (n)
\end{equation}
where
\begin{equation}
  \phi ^{*}_{i} (n) = \beta _i (n+1) \phi _i (n)
\end{equation}
and
\begin{equation}\label{eq:sg56}
  c_{i,n}(s) = \begin{cases}
    1, i=1 \\
    \frac{s h_{n+1}}{\psi _1 (n+1)} = s, i=2 \\
    \left(\frac{sh_{n+1}}{\psi _1(n+1)}\right)
    \left(\frac{sh_{n+1} + \psi _1(n)}{\psi _2(n+1)}\right)
    \dots
    \left(\frac{sh_{n+1} + \psi _{i-2}(n)}{\psi _{i-1}(n+1)}\right), i \ge 3
  \end{cases}
\end{equation}
For variable step sizes, the above becoms (using \autoref{eq:sg54} and \autoref{eq:sg56})
\begin{equation}\label{}
  c_{i,n}(s) = \begin{cases}
    1, i=1 \\
    \alpha _1 (n+1) s = s, i=2 \\
    \left( \alpha _{i-1} (n+1) s + \frac{\psi _{i-2}(n)}{\psi _{i-1}(n+1)} \right) c_{i-1,n}(s) , i \ge 3
  \end{cases}
\end{equation}

Hence, to approximate the derivative of the solution by \autoref{eq:sg52b}, we 
take $s=1$ in \autoref{eq:sg57}, using \autoref{eq:sg54} and \autoref{eq:sg56}
\begin{equation}
  P_{k,n} (x_{n+1}) = p'_{n+1} = \sum^{k}_{i=1} \phi^{*}_{i} (n)
\end{equation}
To approximate the solution at $x_{n+1}$, we can substitute \autoref{eq:sg57} 
into \autoref{eq:sg52a} and integrate to obtain
\begin{equation}\label{eq:sg58}
  p_{n+1} = y_n + h_{n+1} \sum_{i=1}^{k} \phi ^{*}_{i} (n) \int_{0}^{1} c_{i,n} \,ds
\end{equation}
When the step size is constant, \autoref{eq:sg58} reduces to
\begin{equation}
  p_{n+1} = y_n +h \sum_{i=1}^{k} \gamma _{i-1} \nabla ^{i-1} f_n
\end{equation}
If we define the quantity $g_{i,q}=(q-1)!c_{i,n^{(-q)}}$, and 
\begin{equation}\label{eq:sg510}
  g_{i,q} = \begin{cases}
    \frac{1}{q}, \text{ } i=1,\\
    \frac{1}{q(q+1)}, \text{ } i=2,\\
    g_{i-1,q} - \alpha_{i-1}(n+1)g_{i-1,q+1}, \text{ } i \ge 3
  \end{cases}
\end{equation}
then
\begin{equation}\label{eq:sg511}
  p_{n+1} = y_n + h_{n+1} 
    \sum_{i=1}^{k} \phi ^{*}_{i} (n) g_{i,1}
\end{equation}


\subsection{The Corrector}\label{ssec:integrator-corrector}
The code uses a corrector one order higher that the predictor and accepts as an 
approximation to the solution and its derivative at $x_{n=1}$
\begin{equation}
    \begin{aligned}
        y_{n+1} &= y_n + \int_{x_n}^{x_{n+1}} P^{*}_{k+1,n}(t) \,dt \\
        f_{n+1} &= f(x_{n=1}, y_{n+1})
    \end{aligned}
\end{equation}
where
\begin{equation}
    \begin{aligned}
        P^{*}_{k+1,n}(x_{n+1-j}) &= f_{n+1-j}, \text{ } j=1,\dots ,k \\
        P^{*}_{k+1,n}(x_{n+1}) &= f^{p}_{n+1} = f(x_{n+1}, p_{n+1})
    \end{aligned}
\end{equation}
The corrector polynomial interpolates to the same data as the predictor plus 
the additional value $f^{p}_{n+1}$. For the error estimation we also want to consider 
the case of the corrector of order $k$, $P^{*}_{k,n}(x)$. 
Since $P^{*}_{k+1,n}(x)$ interpolates the same data as $P^{*}_{k,n}(x)$ plus one 
extra point, the basic intent of the divided difference form is to represent it as a small 
correction to $P_{k,n}(x)$
\begin{equation}\label{eq:sg512}
    P^{*}_{k+1,n}(x) = P_{k,n}(x) + (x-x_n)(x-x_{n-1})\dots (x-x_{n-k+1})
        f^{p}[x_{n+1}, \dots x_{n-k+1}]
\end{equation}
The superscript $p$ on this divided difference is to remind us that $P^{*}_{k+1,n}(x)$ 
interpolates to $f^{p}_{n+1}$. 
Using alternate notation, this is 
\begin{equation}
    P^{*}_{k+1,n}(x) = P_{k,n}(x) + c_{k+1,n}(s) \phi ^{p}_{k+1} (n+1)
\end{equation}
and after integration we obtain
\begin{equation}\label{eq:sg513}
    \begin{aligned}
        y_{n+1} &= p_{n+1} + h_{n+1} g_{k+1,1} \phi ^{p}_{k+1} (n+1) \\
        f_{n+1} &= f(x_{n+1}, y_{n+1})
    \end{aligned}
\end{equation}
This is a very convenient way to compute $y_{n+1}$ since we can compute the 
coefficient $g_{k+1,1}$ along with the $g_{i,1}$ needed in the prediction process.
if we use a corrector of order $k$ instead of $k+1$ the only change in the formula 
for $y_{n+1}$ is to change $g_{k+1,1}$ to $g_{k,1}$. We shall write $y_{n+1}(k)$ 
to distinguish this value:
\begin{equation}
    y_{n+1}(k) = p_{n+1} + h_{n+1} g_{k,1} \phi ^{p}_{k+1} (n+1)
\end{equation}
These two formulas generalize to variable step size the formulas for constant step size:
\begin{equation}\label{eq:sg513}
    \begin{aligned}
        y_{n+1} &= p_{n+1} + h \gamma _k \nabla ^{k} f^{p}_{n+1} \\
        y_{n+1}(k) &= p_{n+1} + h \gamma _{k-1} \nabla ^{k} f^{p}_{n+1}
    \end{aligned}
\end{equation}
All that remains is to see how to compute the $\phi ^{p}_{i} (n+1)$ and 
$\phi _{i} (n+1)$ to complete the step.
Omitting details we can came up with
\begin{equation}\label{eq:sg514}
    \phi ^{p}_{i+1} (n+1) = \phi ^{p}_{i} (n+1) -  \phi ^{*}_{i} (n)
\end{equation}
and
\begin{equation}\label{eq:sg515}
    \phi _{i+1} (n+1) = \phi _{i} (n+1) -  \phi ^{*}_{i} (n)
\end{equation}
where we note that  $\phi ^{p}_{1} (n+1) = f^{p}_{n+1}$ and 
$\phi _{1} (n+1) = f_{n+1}$ by definition.
It would be ideal if we could form $\phi ^{*}_{i} (n)$ and overwrite $\phi _{i} (n)$. 
To do this we can replace \autoref{eq:sg514} with 
\begin{equation}\label{eq:sg516}
    \phi ^{p}_{i} (n+1) = \phi ^{p}_{i+1} (n+1) + \phi ^{*}_{i} (n)
\end{equation}
recursively in the order $i=k,k-1,\dots ,1$. To start the recursion we can use
\begin{equation}
    \phi ^{p}_{k+1} (n+1) = f^{p}_{n+1} - {p'}_{n+1} = 
    f^{p}_{n+1} - \sum_{i=1}^{k}\phi ^{*}_{i} (n)
\end{equation}
From \autoref{eq:sg514} and \autoref{eq:sg515} we can see that
\begin{equation}\label{eq:sg517}
    \phi _{i} (n+1) = \phi ^{p}_{i} (n+1) + \left( f_{n+1} - \phi ^{p}_{1} (n+1) \right)
\end{equation}
We can find that
\begin{equation}\label{eq:sg518}
    \phi ^{e}_{i} (n+1) = \phi ^{e}_{i+1} (n+1) + \phi ^{*}_{i} (n)
\end{equation}
we can use \autoref{eq:sg518} to generate $\phi ^{e}_{i} (n+1)$ in the sequence 
$i=k, k-1, \dots ,1$ and overwrite them on the $\phi ^{*}_{i} (n)$. 
We can also prove that
\begin{equation}
    \begin{aligned}
        \phi ^{p}_{i} (n+1) &= \phi ^{e}_{i} (n+1) + 
            \left( f^{p}_{n+1} - \phi ^{e}_{1} (n+1) \right) \label{eq:sg519} \\
        \phi _{i} (n+1) &= \phi ^{e}_{i} (n+1) + 
            \left( f_{n+1} - \phi ^{e}_{1} (n+1) \right) \label{eq:sg520}
    \end{aligned}
\end{equation}
Using \autoref{eq:sg519} we generate those $\phi ^{p}_{i} (n+1)$ required for 
the error estimation. If 
the step is successful, we then use \autoref{eq:sg520} to compute the 
$\phi _{i} (n+1)$ and overwrite them on the $\phi ^{e}_{i} (n+1)$.
Let us now summarize all the computation required to advance one step.
The code that is given uses a PECE Adams method which advances from $x_n$ to 
$x_{n+1}$ by
\begin{description}
    \item[Initialize] compute $g_{i,1}$ for $i=1,2, \dots ,k+1$
    \item[P Predicting] 
        \begin{equation}
            \begin{aligned}
                \phi ^{*}_{i} (n) &= \beta _i(n+1) \phi _i(n) \text{ } i=1,2, \dots ,k \\
                p_{n+1} &= y_n + h_{n+1} \sum_{i=1}^{k} g_{i,1} \phi ^{*}_{i} (n) \\
                \phi ^{e}_{k+1} (n+1) &= 0 \\
                \phi ^{e}_{i} (n+1) &= \phi ^{e}_{i+1} (n+1) + \phi ^{*}_{i} (n) \text{ } k,k-1,\dots ,1
            \end{aligned}
        \end{equation}
    \item[E Evaluating] $f^{p}_{n+1} = f(x_{n+1}, p_{n+1})$
    \item[C Correcting] $y_{n+1} = p_{n+1} + h_{n+1} g_{k+1,1} 
        \left( f^{p}_{n+1} - \phi ^{e}_{i} (n+1) \right)$
    \item[E Evaluating]
        \begin{equation}
            \begin{aligned}
                f_{n+1} &= f(x_{n+1}, y_{n+1}) \\
                \phi _{k+1} (n+1) &= f_{n+1} - \phi ^{e}_{1} (n+1) \\
                \phi _{i} (n+1) &= \phi ^{e}_{i} (n+1) + \phi _{k+1} (n+1) \text{ } i=k,k-1,\dots ,1
            \end{aligned}
        \end{equation}
\end{description}
Notice that advancing the differences is very cheap though there may be a 
lot of them. Much of the overhead results from the computation of the $g_{i,1}$.

We stated at the beginning of this chapter that our code would take
advantage of steps taken with a constant size. Let us go into this matter
now and examine the programming of the computation.
Let the variable $n_s$ be the number of successive steps taken with constant 
step size $h$, including the current one. When $\psi _i(n+1)$ is computed it is 
written over the stored value of $\psi _i(n)$.
To compute the $\psi _{i}(n+1)$ we need only start with the value 
$\psi _{n_s}(n+1) = n_s h$ and obtain the remaining elements by the iteration 
\begin{equation}
    \psi _{i}(n+1) = \psi _{i-1}(n) + h \text{ } i= n_s+1 , \dots , k
\end{equation}
and for the $\alpha$ and $\beta$ arrays to advance from $n$ to $n+1$
\begin{equation}
    \begin{aligned}
        \alpha _{n_s} (n+1) &= \frac{1}{n_s} \\
        \alpha _{i} (n+1)   &= \frac{h}{\psi _i(n+1)} \text{ } i= n_s+1, \dots ,k \\
        \beta _{n_s} (n+1)  &= 1 \\
        \beta _{i} (n+1)    &= \beta _{i-1} (n+1) \frac{\psi _{i-1}(n+1)}{\psi _{i-1}(n)} 
            \text{ } i= n_s +1 , \dots ,k
    \end{aligned}
\end{equation}
Obviously the closer we are to working with a constant step size, the less
work need be done in computing these quantities.




[page96]
According to this bound the accuracy is not affected by changes of order.
Variation of the order serves two purposes. One is to start the code and the
other is to achieve the required accuracy at each step as efficiently as
possible.

[page101]
The estimator for the local error $le$, will be
\begin{equation}
  le_{n+1} (k) = u_{n} (x_{n+1}) - y_{n+1}(k) \equiv y_{n+1} - y_{n+1} (k)
\end{equation}
and since
\begin{equation}
  \begin{aligned}
    y_{n+1}    &= p_{n+1} + h_{n+1} g_{k+1,1} \phi ^{p}_{k+1} (n+1) \text{ [C]}\\
    y_{n+1}(k) &= p_{n+1} + h_{n+1} g_{k,1} \phi ^{p}_{k+1} (n+1)  \text{ [P]}
  \end{aligned}
\end{equation}
lead to 
\begin{equation}
  le_{n+1} (k) \equiv h_{n+1} \left( g_{k+1,1} - g_{k,1} \right) \phi ^{p}_{k+1} (n+1)
\end{equation}
\fi
