\section{Integrator implementation}\label{sec:integrator-implementation}

\iffalse
\fi

In this section a brief outline of the integrator designed and implemented for 
the current Thesis is given. The method used, is a \emph{variable step}, 
\emph{variable order} Adams-Bashforth-Moulton \gls{pece} algorithm. The 
implementation is based on the developments of \cite{Shampine1975}, which up to 
date constitute one of the most robust \gls{ode} solvers solvers, given the 
problem constraints (non stiff \gls{ode} systems of 1\textsuperscript{st} order).
An overview of the implemented schema is depicted in \autoref{fig:sgode}.

According to \cite{Shampine1975}, use of Adams methods are most advantageous for 
problems in which:
\begin{itemize}\setlength\itemsep{.3em}
    \item function evaluations are expensive,
    \item moderate to high accuracy is requested,
    \item many output points are required
\end{itemize}

It is worth noting that  major scientific software packages used for \gls{pod}, such
as \texttt{GINS}, developed by \gls{cnes} and \gls{grgs} (\cite{Gins2013}) and 
\texttt{GEODYN} developed by \gls{gsfc} (\cite{Geodyn2015}) use \emph{multi-step} 
methods for the numerical integration. The algorithm implementation was written 
from scratch requiring many difficult steps, as explained in the preceding sections. 
Exisiting libraries were not preferred (when available) as were developed with 
reduced analytical capabilities for modern computing systems. An overview of the 
integrator designed for this Thesis, is given in \autoref{fig:sgode}. 

In \gls{pod}, the cost of calling the derivative function $\bm{f}(t, \bm{y}(t))$ 
is quite heavy and repeated calls can make the integrator inefficient. Hence, care 
must be taken to limit such computations. The Adams methods, when carefully used, 
are more efficient in this respect than any other method being used today to 
solve general \gls{ode} (\cite{Shampine1975}). This incurs an additional overhead,
however, because making such decisions necessitates estimating the errors that 
are, or would be, incurred for various step sizes and orders. To address this 
issue, most steps are performed in groups with constant step size and order.

\begin{figure}
  \centering
  \input{tikz/sgode}
  \caption{Flow chart of the implemented \gls{pece} integrator, based 
    on \cite{Shampine1975}.}
  \label{fig:sgode}
\end{figure}

For a varying step size, a wise choice of stored variables and coefficients 
should be made to accommodate efficiency. Since, the algorithm will be using divided 
and backward differences, this choice includes the variables

\begin{gather}
  h_i  = x_i - x_{i-1}, \label{eq:sg4a}\\
  s    = \frac{x - x_n}{h_{n+1}}, \label{eq:sg4b}\\
  \psi _i (n+1)   = h_{n+1} + h_{n} + \dots + h_{n+2-i} \text{ } i \ge 1, \label{eq:sg4c}\\
  \alpha _i (n+1) = \frac{h_{n+1}}{\psi _i (n+1)} \text{ } i \ge 1, \label{eq:sg4d}\\
  \beta _1 (n+1)  = 1, \label{eq:sg4e}\\
  \beta _i (n+1)  = \frac{\psi _1 (n+1) \psi _2 (n+1) \dots \psi _{i-1}(n+1)}{\psi _1 (n) \psi _2 (n) \dots \psi _{i-1}(n)} \text{ } i > 1, \label{eq:sg4f}\\
  \phi _1(n) = f[x_n] = f_n, \label{eq:sg4g}\\
  \phi _i(n) = \psi _1 (n) \psi _2 (n) \dots \psi _{i-1}(n) f[x_n,x_{n-1},\dots ,x_{n-i+1}], \text{ } i > 1 \label{eq:sg4h}
\end{gather}
where $h_i$ are the step size, 
$\psi _i(n+1) = h_{n+1} + h_{n} + \dots + h_{n+2-i} = x_{n+1}-x{n+1-i}$ are the 
sums of the step sizes, $\phi _{i}(n)$ are the modified divided differences (which 
reduce to backward differences for constant step sizes), $s$ is a normalized variable 
taking value in the range $[0,1]$ as x runs from $x_n$ to $x_{n+1}$ and $\alpha _i$ 
and $\beta _i$ are intermediate coefficients used for the computations.

Through \autoref{eq:sg4a} to \autoref{eq:sg4h}, a change from value $n$ to $n+1$ 
can be performed via the recursion for $i=2,3,\dots$
\begin{equation}
  \begin{aligned}
    \psi _i(n+1)   &= \psi _{i-1}(n) + h_{n+1} \\
    \beta _i(n+1)  &= \beta _{i-1}(n+1) \frac{\psi _{i-1}(n+1)}{\psi _{i-1}(n)}
  \end{aligned}
\end{equation}
with initial conditions for $i=1$
\begin{equation}
  \begin{aligned}
    \psi _1(n+1)   &= h_{n+1} \\
    \beta _1(n+1)  &= 1
  \end{aligned}
\end{equation}

It is worth noting that if the step size is constant, \autoref{eq:sg4a} through 
\autoref{eq:sg4h} are simplified to
\begin{gather}
  s = \frac{x-x_n}{h_{n+1}} , \label{eq:sg4Ab}\\
  \psi _i (n+1)   = ih \label{eq:sg4Ac}\\
  \alpha _i (n+1) = \frac{1}{i} \label{eq:sg4Ad}\\
  \beta_i (n+1)   = 1 \label{eq:sg4Ae}\\
  \phi _i(n)      = \nabla ^{i-1} f_n, \label{eq:sg4g}
\end{gather}

The expression of the polynomial $P_{k,n}(x)$ in \autoref{eq:sg52a} and \autoref{eq:sg52b} 
can be written as
\begin{equation}\label{eq:sq57}
  P_{k,n}(t) = \sum_{i=1}^{k} c_{i,n} (s) \phi ^{*}_{i}(n)
\end{equation}
where
\begin{align}
  c_{i,n}(s) &= \begin{cases}
    1                                    & i=1 \\
    \frac{s h_{n+1}}{\psi _1 (n+1)} = s  & i=2 \\
    \left(\frac{sh_{n+1}}{\psi _1(n+1)}\right)
    \left(\frac{sh_{n+1} + \psi _1(n)}{\psi _2(n+1)}\right)
    \dots
    \left(\frac{sh_{n+1} + \psi _{i-2}(n)}{\psi _{i-1}(n+1)}\right) & i \ge 3
  \end{cases}
  \label{eq:sg56} \\
  \phi ^{*}_{i}(n) &= \beta _i (n+1) \phi _i (n) \label{eq:sg56b}
\end{align}
The derivative in \autoref{eq:sg52b}, is then
\begin{equation}\label{eq:sg57b}
   P_{k,n}(t_{n+1}) = {p'}_{n+1} = \sum_{i=1}^{k} \phi ^{*}_{i}(n)
\end{equation}
and the (approximate) solution at $t_{n+1}$, \autoref{eq:sg52a} is
\begin{equation}\label{eq:sg58}
  p_{n+1} = y_n + h_{n+1} \sum_{i=1}^{k} \phi ^{*}_{i}(n) \int_{0}^{1} c_{i,n}(s) \,ds
\end{equation}
As already shown, for constant step size $h$, \autoref{eq:sg58} reduces to
\begin{equation}\label{eq:sg58b}
  p_{n+1} = y_n + h \sum_{i=1}^{k} \gamma _{i-1} \nabla ^{i-1} f_n 
\end{equation}
To compute the integral in \autoref{eq:sg58}, the quantity $g_{i,q}$ is 
introduced, defined as $g_{i,q} = (q-1)! c_{i,n}^{(-q)}(1)$. According to 
\cite{Shampine1975}, the integral can now be evaluated via the formula
\begin{equation}
    \int_{0}^{1} c_{i,n}(s) \,ds = \sum_{i=1}^{k} g_{i,1} \phi ^{*}_{i}(n)
\end{equation}
so that \autoref{eq:sg58} can be numerically computed from
\begin{equation}\label{eq:sg511}
  p_{n+1} = y_n + h_{n+1}  \sum_{i=1}^{k} g_{i,1} \phi ^{*}_{i}(n)
\end{equation}
The coefficients $g_{i,q}$ follow the recursion formulas
\begin{equation}\label{eq:sg510}
  g_{i,q} = \begin{cases}
    \frac{1}{q} & i=1 \\
    \frac{1}{q(q+1)} & i=2 \\
    g_{i-1,q} - \alpha _{i-1}(n+1) g_{i-1,q+1} & i \ge 3
  \end{cases}
\end{equation}

In the formulas presented here, a series of simplifications can be made if a 
constant step size $h$ is considered. These are implemented in the source code for 
the integrator. The interested reader can find details in \cite{Shampine1975}.

\iffalse
\cite{Shampine1975}:
The overhead in these codes is rather high, partly because of the methods
and partly because of the features they provide. For example, DE detects
and deals with discontinuities; it detects and deals with moderate stiffness;
it detects and tells the user of severe stiffness; it detects requests for very
high accuracy and switches on propagated roundoff controls; it detects
requests for more accuracy than is possible on the machine being used and
tells the user what is possible; it is, for practical purposes, independent
of the number and location of output points; it monitors the work being
expended; it allows the user to change direction without restarting; and
it is extremely efficient in terms of function evaluations. 
\fi

\subsection{The Predictor}\label{ssec:integrator-predictor}
The predictor part of the \gls{pece} integrator follows the Adams-Bashforth 
method (see \autoref{ssec:adams-method}). In general, a $k$th order Adams-Bashforth 
predictor at $t_n$ is defined by the expression 
\begin{equation}\label{eq:sg51}
  y_n + \int_{t_n}^{t} P_{k,n}(t) \,dt
\end{equation}
where $P_{k,n}(t)$ satisfies the interpolation conditions
\begin{equation}\label{eq:sg51b}
  P_{k,n}(t_{n+1-j}) = \bm{f}_{n+1-j} \text{ for } j=1,\dots ,k
\end{equation}
\autoref{eq:sg51} is used to predict both the solution and its derivative at 
$t_{n+1}$
\begin{align}
  p_{n+1}    &= y_n + \int_{t_n}^{t_{n+1}}  P_{k,n} (t) \,dt \label{eq:sg52a}\\
  {p'}_{n+1} &= P_{n,k}(t_{n+1}) \label{eq:sg52b}
\end{align}
The implementation computes the above values using \autoref{eq:sg511} and 
\autoref{eq:sg57b} using a series of intermediate variables and coefficients (
as described in \autoref{sec:integrator-implementation}). To promote efficiency, 
these coefficients are stacked in contiguous memory, divided in blocks based on 
their usage in the algorithm, to minimize as much as possible cache misses.

\subsection{The Corrector}\label{ssec:integrator-corrector}
The corrector part of the \gls{pece} integrator is based on the Adams-Moulton 
algorithm. In this step, $p_{n+1}$ is ``corrected'' and all relevant values 
needed for the next step are computed and stored. The corrector implemented here, 
is one order higher than the predictor. In general, the solution and its derivative, 
using an Adams-Moulton method, is given by
\begin{equation}\label{eq:sg5cor1}
  y_{n+1} = y_n + \int_{t_n}^{t_{n+1}} P^{*}_{k+1,n}(t) \,dt
\end{equation}
and
\begin{equation}\label{eq:sg51b}
  f_{n+1} = f(t_{n+1}, y_{n+1})
\end{equation}
where 
\begin{align}
  P^{*}_{k+1,n}(t_{n+1-j}) &= f_{n+1-j} \mbox{ for } j=1,2,\dots ,k \\
  P^{*}_{k+1,n}(t_{n+1})   &= f^{p}_{n+1} = f(t_{n+1}, p_{n+1})
\end{align}
The corrector polynomial interpolates the same set of values as the predictor, plus 
the additional value $f^{p}_{n+1}$. Using the divided differences formulation, 
$P^{*}_{k+1,n}$ can be computed from $P_{k,n}$ using one extra term. Using the 
notation introduced in \autoref{sec:integrator-implementation}, the polynomial 
can be given by
\begin{equation}
  P^{*}_{k+1,n}(t) = P_{n,k}(t) + c_{k+1,n}(s) \phi ^{p}_{k+1}(n+1)
\end{equation}
and after integration, the solution and derivative function can be shown to be 
(\cite{Shampine1975})
\begin{align}
  y_{n+1} &= p_{n+1} + h_{n+1} g_{k+1,1} \phi ^{p}_{k+1} (n+1) \label{eq:sg513a} \\
  f_{n+1} &= f(t_{n+1}, y_{n+1}) \label{eq:sg513b}
\end{align}
\autoref{eq:sg513a} allows the computation of the ``corrected'' solution using the 
work done at the corrector phase, plus the term $g_{k+1,1}$; the latter is computed 
along with the $g_{i,1}$, needed in the prediction (see \autoref{eq:sg511}). Note 
also that changing the term $g_{k+1,1}$ to $g_{k,1}$, yields the solution of 
order $k$ (\cite{Shampine1975}), a fact used to assist the estimation of the error 
(here the notation $y_{n+1}(k)$ is used to distinguish between the order $k$ and 
solution the order $k+1$ as given in \autoref{eq:sg513a})
\begin{equation}\label{eq:sg513ak}
  y_{n+1}(k) = p_{n+1} + h_{n+1} g_{k,1} \phi ^{p}_{k+1} (n+1)
\end{equation}
In the case of constant step size $h$, \autoref{eq:sg513a} and \autoref{eq:sg513ak} 
reduce to
\begin{align}
  y_{n+1}    &= p_{n+1} + h \gamma _k \nabla ^{k} f^{p}_{n+1} \\
  y_{n+1}(k) &= p_{n+1} + h \gamma _{k-1} \nabla ^{k} f^{p}_{n+1}
\end{align}
The terms $\phi ^{p}_{i} (n+1)$ and $\phi _{i} (n+1)$ can be computed via recursion, 
given that the divided difference approach is followed, from (\cite{Shampine1975})
\begin{align}
  \phi ^{p}_{i+1} (n+1) &= \phi ^{p}_{i} (n+1) - \phi ^{*}_{i} (n) \mbox{  for } i=1,2,\dots ,k \label{eq:sg514}\\
  \phi _{i+1} (n+1)     &= \phi _{i} (n+1) - \phi ^{*}_{i} (n)     \mbox{  for } i=1,2,\dots ,k \label{eq:sg515}
\end{align}
with initial conditions
\begin{align}
  \phi ^{p}_{1} (n+1) = f_{n+1}^{p} \\
  \phi _{1} (n+1) = f_{n+1}
\end{align}
According to \cite{Shampine1975}, despite the straightforwardness of these recursive 
relations, they lack storage efficiency. This can be corrected using an alternate way 
to compute these terms. Introducing the modified divided differences $\phi ^{e}_{i}$, 
that make us of the value ${p'}_{n+1}$ at $t_{n+1}$ (that is extend the differences 
$\phi _i(n)$ based on the sequence $x_n, x_{n-1}, \dots$ to the values at 
$x_{n+1}, x_{n}, x_{n-1} \dots$)
\begin{equation}\label{eq:sg518}
  \phi ^{e}_{i}(n+1) = \phi ^{e}_{i+1} (n+1) + \phi^{*}_{i} (n)
\end{equation}
which can be used to generate the $\phi ^{e}_{i}(n+1)$ terms in the sequence 
$i=k, k-1, \dots ,1$. Now, \autoref{eq:sg514} and \autoref{eq:sg515} can be written as
\begin{align}
  \phi ^{p}_{i} (n+1) &= \phi ^{e}_{i} (n+1) - \left( f^{p}_{n+1} - \phi ^{e}_{1} (n+1) \right) \label{eq:sg519}\\
  \phi _{i} (n+1)     &= \phi ^{e}_{i} (n+1) - \left( f^{p}_{n+1} - \phi ^{e}_{1} (n+1) \right) \label{eq:sg520}
\end{align}

The combined predictor-corrector step phase is depicted in \autoref{fig:adams-pece}.
\begin{figure}
  \centering
  \input{tikz/adams-pece}
  \caption{Schematic representation of the Adams \gls{pece} integrator, based 
    on \cite{Shampine1975}.}
  \label{fig:adams-pece}
\end{figure}

\subsection{Error Estimation, Order and Step Size}\label{ssec:integrator-error-estimation}
Error estimation in the integration process plays a crucial role, since it allows 
the algorithm to adjust the step size and order accordingly, so as to preserve 
efficiency and accuracy. Local error can be computed before the final step of the 
\gls{pece} algorithm, aka after the corrector, for orders $k-2$, $k-1$, $k$ and 
$k+1$. Hence, if the step is unsuccessful, the final evaluation is not performed, 
and a change of the order is examined.

In the source code the quantity $ERK$ is used to make decisions on the step size. 
This quantity was introduced by \cite{Shampine1975} and is given by
\begin{equation}
  ERK = \norm{h \left( g_{k+1,1} - g_{k,1} \right) \sigma _{k+1}(n+1) \phi ^{p}_{k+1}(n+1)}
\end{equation}
In reality, $ERK$ estimates what the local error at $x_{n+1}$ would have been, 
had the preceding steps been of constant size $h$. Given a successful step, the 
next step (to $x_{n+2}$) of size $rh$, will have a local error of $r^{k+1} ERK$. 
If on contrary the step were to be repeated with a step size of $rh$, then again 
its error estimate would be $r^{k+1} ERK$.

For order selection, three further quantities are introduced, based on \cite{Shampine1975}
\begin{align}
  ERKM1 &= \norm{h \left( g_{k+1,1} - g_{k,1} \right) \sigma _{k}(n+1) \phi ^{p}_{k}(n+1)} \\
  ERKM2 &= \norm{h \left( g_{k+1,1} - g_{k,1} \right) \sigma _{k-1}(n+1) \phi ^{p}_{k-1}(n+1)}\\
  ERKP1 &= \norm{h \left( g_{k+1,1} - g_{k,1} \right) \phi _{k+2}(n+1)}
\end{align}
The first two, estimate what the local error would have been at $x_{n+1}$ had these 
steps be taken with an order of $k-1$ and $k-2$ respectively. $ERKP1$ is an estimate 
of the local error at $x_{n+2}$. All of the estimates assume a step size $h$. 
The philosophy of the order selection is to change the order only if the predicted 
error is reduced and if there is a trend in the terms. In general, lower order 
formulas have better stability properties (\cite{Shampine1975}), and the algorithm 
is so designed as to prefer lower orders. The maximum order limit within the 
algorithm is 12.

After the order selection, the algorithm tries to select the optimal step size $rh$. 
The value of $r$ is initially estimated by
\begin{equation}\label{eq:sg71}
  r = \left( \frac{0.5 \epsilon}{ERK} \right) ^{\frac{1}{k+1}}
\end{equation}
and restrict increase of $r$ to factors of two. Hence, if the inequality 
$0.5 \epsilon \ge 2^{k+1}ERK$, holds, the step size is doubled. If this is not 
possible, then the inequality $0.5 \epsilon \ge ERK$ is checked, to see if a constant 
step size can be retained. Keeping a constant step size (when a factor of two 
increase is not feasible) offers efficiency, simplicity, stability and a more 
realistic error estimation. If however the latter inequality demands so, a decrease 
of step size is performed by a factor in between 0.5 to 0.9, computed via \autoref{eq:sg71}.
The value of $\epsilon$ is user defined.

\subsection{Design Considerations}\label{ssec:design-considerations}
An efficient and precise solution of the orbit integration problem plays a key 
role in \gls{pod} applications. The algorithm described above is a state-of-the-art 
integrator, which allows for a robust treatment of the problem at hand. 
A few design consideration are discussed here, which enable further enhancements 
of the algorithm's efficiency based on its architectural design.

The whole algorithm is encapsulated in an \emph{object-oriented} design; this provides easy and 
user-friendly interaction with the integration process and eliminates in as much as 
possible errors incurred by user misusage. One additional benefit is that the 
user (or the user application) does not need to know the very complex background and 
methods that are implemented within the algorithm. One only needs to specify the 
initial conditions of the \gls{ode} and if needed the tolerance values (see 
\autoref{ssec:integrator-error-estimation}). The algorithm will then seek for the 
optimal way (i.e. stepsize and order) to handle the problem.

Additionally, the object oriented design implemented enables the integrator instance (i.e. 
a data structure of integrator type) to have \emph{state}. Users can query this state 
and find out information about the process, a fact that enables a clear and unambiguous 
interaction between the instance and the caller.

Memory allocations are handled using the \emph{Resource Acquisition Is Initialization} (RAII) 
programming technique (\url{https://en.cppreference.com/w/cpp/language/raii}) and 
users are free of any obligation regarding allocating/freeing memory. Special care is 
taken so that only no excess memory is allocated and most importantly, that memory blocks 
are allocated in a contiguous manner, minimizing cache misses.

Part of the source code uses the \texttt{eigen} (\cite{eigenweb}) library, which 
supports vectorization (\emph{Single Instruction, Multiple Data} (SIMD) instructions) 
including the widely used \emph{Advanced Vector Extensions} AVX, AVX2 and AVX512.
